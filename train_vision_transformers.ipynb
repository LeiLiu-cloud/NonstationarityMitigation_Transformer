{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ll34246\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from ViT import ViT\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import wandb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an example of training data x and label y; \n",
    "# practical case should have more tarining data and split train/test sets\n",
    "x = np.random.rand(1,224,224,1) \n",
    "y = np.random.rand(1,1)\n",
    "\n",
    "#transfer to torch.tensor\n",
    "x = torch.from_numpy(x.reshape(1,1,224,224)).float()\n",
    "y = torch.from_numpy(y.reshape(-1,1)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mydata(Dataset):\n",
    "    \n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image = self.data[index]\n",
    "        label = self.targets[index]\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "dataset = Mydata(x,y)\n",
    "train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True) \n",
    "val_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)  \n",
    "##########################################\n",
    "### below is for train/test data split ###\n",
    "##########################################\n",
    "\n",
    "#train_size = int(0.7*len(dataset))\n",
    "#test_size = len(dataset) - train_size\n",
    "#train_set, val_set = random_split(dataset, [train_size, test_size]) \n",
    "\n",
    "#train_dataloader = DataLoader(train_set, batch_size=32, shuffle=True) \n",
    "#val_dataloader = DataLoader(val_set, batch_size=32, shuffle=True)     \n",
    "\n",
    "#train_features, train_labels = next(iter(train_dataloader))\n",
    "#print(f\"Feature batch shape: {train_features.size()}\")\n",
    "#print(f\"Labels batch shape: {train_labels.size()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Vision Transformers (here we use ViT as an example, train SwinT can follow similar way)\n",
    "> Step1: load ViT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViT(\n",
       "  (to_patch_embedding): Sequential(\n",
       "    (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=8, p2=8)\n",
       "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (transformer): Transformer(\n",
       "    (layers): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Attention(\n",
       "            (attend): Softmax(dim=-1)\n",
       "            (to_qkv): Linear(in_features=32, out_features=576, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=192, out_features=32, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Attention(\n",
       "            (attend): Softmax(dim=-1)\n",
       "            (to_qkv): Linear(in_features=32, out_features=576, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=192, out_features=32, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Attention(\n",
       "            (attend): Softmax(dim=-1)\n",
       "            (to_qkv): Linear(in_features=32, out_features=576, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=192, out_features=32, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Attention(\n",
       "            (attend): Softmax(dim=-1)\n",
       "            (to_qkv): Linear(in_features=32, out_features=576, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=192, out_features=32, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Attention(\n",
       "            (attend): Softmax(dim=-1)\n",
       "            (to_qkv): Linear(in_features=32, out_features=576, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=192, out_features=32, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Attention(\n",
       "            (attend): Softmax(dim=-1)\n",
       "            (to_qkv): Linear(in_features=32, out_features=576, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=192, out_features=32, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (to_latent): Identity()\n",
       "  (mlp_head): Sequential(\n",
       "    (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train with GPU otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#load model (play with model architectures such as depth, patch size, number of heads...)\n",
    "model = ViT(\n",
    "            image_size = 224,\n",
    "            patch_size = 8,\n",
    "            num_classes = 1,\n",
    "            dim = 32, \n",
    "            depth = 6,\n",
    "            heads = 3,\n",
    "            mlp_dim = 128, \n",
    "            dropout = 0.0,\n",
    "            emb_dropout = 0.0\n",
    "            )\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> step2: define training parameters and train ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------This is the 1 epoch training---------\n",
      "total train loss:0.42194271087646484\n",
      "time elapsed: 1.5211s\n",
      "total val loss:0.4207490384578705\n",
      "----------This is the 2 epoch training---------\n",
      "total train loss:0.4207490384578705\n",
      "time elapsed: 1.5531s\n",
      "total val loss:0.4195575416088104\n",
      "----------This is the 3 epoch training---------\n",
      "total train loss:0.4195575416088104\n",
      "time elapsed: 1.5771s\n",
      "total val loss:0.41836708784103394\n",
      "----------This is the 4 epoch training---------\n",
      "total train loss:0.41836708784103394\n",
      "time elapsed: 1.6081s\n",
      "total val loss:0.41717857122421265\n",
      "----------This is the 5 epoch training---------\n",
      "total train loss:0.41717857122421265\n",
      "time elapsed: 1.6381s\n",
      "total val loss:0.4159916043281555\n",
      "----------This is the 6 epoch training---------\n",
      "total train loss:0.4159916043281555\n",
      "time elapsed: 1.6720s\n",
      "total val loss:0.4148068428039551\n",
      "----------This is the 7 epoch training---------\n",
      "total train loss:0.4148068428039551\n",
      "time elapsed: 1.6990s\n",
      "total val loss:0.41362348198890686\n",
      "----------This is the 8 epoch training---------\n",
      "total train loss:0.41362348198890686\n",
      "time elapsed: 1.7280s\n",
      "total val loss:0.4124418795108795\n",
      "----------This is the 9 epoch training---------\n",
      "total train loss:0.4124418795108795\n",
      "time elapsed: 1.7620s\n",
      "total val loss:0.4112623631954193\n",
      "----------This is the 10 epoch training---------\n",
      "total train loss:0.4112623631954193\n",
      "time elapsed: 1.8020s\n",
      "total val loss:0.41008442640304565\n",
      "----------This is the 11 epoch training---------\n",
      "total train loss:0.41008442640304565\n",
      "time elapsed: 1.8320s\n",
      "total val loss:0.40890827775001526\n",
      "----------This is the 12 epoch training---------\n",
      "total train loss:0.40890827775001526\n",
      "time elapsed: 1.8659s\n",
      "total val loss:0.40773215889930725\n",
      "----------This is the 13 epoch training---------\n",
      "total train loss:0.40773215889930725\n",
      "time elapsed: 1.8999s\n",
      "total val loss:0.40655770897865295\n",
      "----------This is the 14 epoch training---------\n",
      "total train loss:0.40655770897865295\n",
      "time elapsed: 1.9349s\n",
      "total val loss:0.40538525581359863\n",
      "----------This is the 15 epoch training---------\n",
      "total train loss:0.40538525581359863\n",
      "time elapsed: 1.9649s\n",
      "total val loss:0.4042133092880249\n",
      "----------This is the 16 epoch training---------\n",
      "total train loss:0.4042133092880249\n",
      "time elapsed: 1.9979s\n",
      "total val loss:0.40304332971572876\n",
      "----------This is the 17 epoch training---------\n",
      "total train loss:0.40304332971572876\n",
      "time elapsed: 2.0298s\n",
      "total val loss:0.40187522768974304\n",
      "----------This is the 18 epoch training---------\n",
      "total train loss:0.40187522768974304\n",
      "time elapsed: 2.0558s\n",
      "total val loss:0.4007090926170349\n",
      "----------This is the 19 epoch training---------\n",
      "total train loss:0.4007090926170349\n",
      "time elapsed: 2.0878s\n",
      "total val loss:0.3995450437068939\n",
      "----------This is the 20 epoch training---------\n",
      "total train loss:0.3995450437068939\n",
      "time elapsed: 2.1188s\n",
      "total val loss:0.3983830511569977\n",
      "----------This is the 21 epoch training---------\n",
      "total train loss:0.3983830511569977\n",
      "time elapsed: 2.1578s\n",
      "total val loss:0.3972235321998596\n",
      "----------This is the 22 epoch training---------\n",
      "total train loss:0.3972235321998596\n",
      "time elapsed: 2.1898s\n",
      "total val loss:0.396065890789032\n",
      "----------This is the 23 epoch training---------\n",
      "total train loss:0.396065890789032\n",
      "time elapsed: 2.2167s\n",
      "total val loss:0.39491087198257446\n",
      "----------This is the 24 epoch training---------\n",
      "total train loss:0.39491087198257446\n",
      "time elapsed: 2.2557s\n",
      "total val loss:0.39375796914100647\n",
      "----------This is the 25 epoch training---------\n",
      "total train loss:0.39375796914100647\n",
      "time elapsed: 2.2927s\n",
      "total val loss:0.39261019229888916\n",
      "----------This is the 26 epoch training---------\n",
      "total train loss:0.39261019229888916\n",
      "time elapsed: 2.3327s\n",
      "total val loss:0.3914645314216614\n",
      "----------This is the 27 epoch training---------\n",
      "total train loss:0.3914645314216614\n",
      "time elapsed: 2.3697s\n",
      "total val loss:0.3903212249279022\n",
      "----------This is the 28 epoch training---------\n",
      "total train loss:0.3903212249279022\n",
      "time elapsed: 2.4096s\n",
      "total val loss:0.38918033242225647\n",
      "----------This is the 29 epoch training---------\n",
      "total train loss:0.38918033242225647\n",
      "time elapsed: 2.4446s\n",
      "total val loss:0.3880417048931122\n",
      "----------This is the 30 epoch training---------\n",
      "total train loss:0.3880417048931122\n",
      "time elapsed: 2.4806s\n",
      "total val loss:0.3869054913520813\n",
      "----------This is the 31 epoch training---------\n",
      "total train loss:0.3869054913520813\n",
      "time elapsed: 2.5146s\n",
      "total val loss:0.38577166199684143\n",
      "----------This is the 32 epoch training---------\n",
      "total train loss:0.38577166199684143\n",
      "time elapsed: 2.5386s\n",
      "total val loss:0.3846401870250702\n",
      "----------This is the 33 epoch training---------\n",
      "total train loss:0.3846401870250702\n",
      "time elapsed: 2.5625s\n",
      "total val loss:0.3835110068321228\n",
      "----------This is the 34 epoch training---------\n",
      "total train loss:0.3835110068321228\n",
      "time elapsed: 2.5925s\n",
      "total val loss:0.3823842406272888\n",
      "----------This is the 35 epoch training---------\n",
      "total train loss:0.3823842406272888\n",
      "time elapsed: 2.6195s\n",
      "total val loss:0.381259948015213\n",
      "----------This is the 36 epoch training---------\n",
      "total train loss:0.381259948015213\n",
      "time elapsed: 2.6485s\n",
      "total val loss:0.38013792037963867\n",
      "----------This is the 37 epoch training---------\n",
      "total train loss:0.38013792037963867\n",
      "time elapsed: 2.6745s\n",
      "total val loss:0.37901854515075684\n",
      "----------This is the 38 epoch training---------\n",
      "total train loss:0.37901854515075684\n",
      "time elapsed: 2.6985s\n",
      "total val loss:0.37790125608444214\n",
      "----------This is the 39 epoch training---------\n",
      "total train loss:0.37790125608444214\n",
      "time elapsed: 2.7225s\n",
      "total val loss:0.3767864406108856\n",
      "----------This is the 40 epoch training---------\n",
      "total train loss:0.3767864406108856\n",
      "time elapsed: 2.7464s\n",
      "total val loss:0.37567391991615295\n",
      "----------This is the 41 epoch training---------\n",
      "total train loss:0.37567391991615295\n",
      "time elapsed: 2.7814s\n",
      "total val loss:0.37456390261650085\n",
      "----------This is the 42 epoch training---------\n",
      "total train loss:0.37456390261650085\n",
      "time elapsed: 2.8094s\n",
      "total val loss:0.3734559714794159\n",
      "----------This is the 43 epoch training---------\n",
      "total train loss:0.3734559714794159\n",
      "time elapsed: 2.8440s\n",
      "total val loss:0.37235042452812195\n",
      "----------This is the 44 epoch training---------\n",
      "total train loss:0.37235042452812195\n",
      "time elapsed: 2.8710s\n",
      "total val loss:0.37124723196029663\n",
      "----------This is the 45 epoch training---------\n",
      "total train loss:0.37124723196029663\n",
      "time elapsed: 2.9039s\n",
      "total val loss:0.3701471984386444\n",
      "----------This is the 46 epoch training---------\n",
      "total train loss:0.3701471984386444\n",
      "time elapsed: 2.9359s\n",
      "total val loss:0.3690529465675354\n",
      "----------This is the 47 epoch training---------\n",
      "total train loss:0.3690529465675354\n",
      "time elapsed: 2.9659s\n",
      "total val loss:0.3679615259170532\n",
      "----------This is the 48 epoch training---------\n",
      "total train loss:0.3679615259170532\n",
      "time elapsed: 3.0039s\n",
      "total val loss:0.3668724298477173\n",
      "----------This is the 49 epoch training---------\n",
      "total train loss:0.3668724298477173\n",
      "time elapsed: 3.0289s\n",
      "total val loss:0.36578574776649475\n",
      "----------This is the 50 epoch training---------\n",
      "total train loss:0.36578574776649475\n",
      "time elapsed: 3.0609s\n",
      "total val loss:0.36470192670822144\n",
      "----------This is the 51 epoch training---------\n",
      "total train loss:0.36470192670822144\n",
      "time elapsed: 3.0858s\n",
      "total val loss:0.36362048983573914\n",
      "----------This is the 52 epoch training---------\n",
      "total train loss:0.36362048983573914\n",
      "time elapsed: 3.1098s\n",
      "total val loss:0.36254122853279114\n",
      "----------This is the 53 epoch training---------\n",
      "total train loss:0.36254122853279114\n",
      "time elapsed: 3.1398s\n",
      "total val loss:0.36146456003189087\n",
      "----------This is the 54 epoch training---------\n",
      "total train loss:0.36146456003189087\n",
      "time elapsed: 3.1708s\n",
      "total val loss:0.360390305519104\n",
      "----------This is the 55 epoch training---------\n",
      "total train loss:0.360390305519104\n",
      "time elapsed: 3.1948s\n",
      "total val loss:0.35931843519210815\n",
      "----------This is the 56 epoch training---------\n",
      "total train loss:0.35931843519210815\n",
      "time elapsed: 3.2188s\n",
      "total val loss:0.35824912786483765\n",
      "----------This is the 57 epoch training---------\n",
      "total train loss:0.35824912786483765\n",
      "time elapsed: 3.2427s\n",
      "total val loss:0.35718223452568054\n",
      "----------This is the 58 epoch training---------\n",
      "total train loss:0.35718223452568054\n",
      "time elapsed: 3.2667s\n",
      "total val loss:0.3561176359653473\n",
      "----------This is the 59 epoch training---------\n",
      "total train loss:0.3561176359653473\n",
      "time elapsed: 3.2937s\n",
      "total val loss:0.3550553321838379\n",
      "----------This is the 60 epoch training---------\n",
      "total train loss:0.3550553321838379\n",
      "time elapsed: 3.3177s\n",
      "total val loss:0.3539976477622986\n",
      "----------This is the 61 epoch training---------\n",
      "total train loss:0.3539976477622986\n",
      "time elapsed: 3.3417s\n",
      "total val loss:0.3529427647590637\n",
      "----------This is the 62 epoch training---------\n",
      "total train loss:0.3529427647590637\n",
      "time elapsed: 3.3657s\n",
      "total val loss:0.35188981890678406\n",
      "----------This is the 63 epoch training---------\n",
      "total train loss:0.35188981890678406\n",
      "time elapsed: 3.3937s\n",
      "total val loss:0.35083961486816406\n",
      "----------This is the 64 epoch training---------\n",
      "total train loss:0.35083961486816406\n",
      "time elapsed: 3.4206s\n",
      "total val loss:0.3497912287712097\n",
      "----------This is the 65 epoch training---------\n",
      "total train loss:0.3497912287712097\n",
      "time elapsed: 3.4446s\n",
      "total val loss:0.3487454354763031\n",
      "----------This is the 66 epoch training---------\n",
      "total train loss:0.3487454354763031\n",
      "time elapsed: 3.4686s\n",
      "total val loss:0.34770193696022034\n",
      "----------This is the 67 epoch training---------\n",
      "total train loss:0.34770193696022034\n",
      "time elapsed: 3.4976s\n",
      "total val loss:0.3466605246067047\n",
      "----------This is the 68 epoch training---------\n",
      "total train loss:0.3466605246067047\n",
      "time elapsed: 3.5216s\n",
      "total val loss:0.3456212878227234\n",
      "----------This is the 69 epoch training---------\n",
      "total train loss:0.3456212878227234\n",
      "time elapsed: 3.5486s\n",
      "total val loss:0.3445844054222107\n",
      "----------This is the 70 epoch training---------\n",
      "total train loss:0.3445844054222107\n",
      "time elapsed: 3.5766s\n",
      "total val loss:0.3435501456260681\n",
      "----------This is the 71 epoch training---------\n",
      "total train loss:0.3435501456260681\n",
      "time elapsed: 3.6075s\n",
      "total val loss:0.3425177335739136\n",
      "----------This is the 72 epoch training---------\n",
      "total train loss:0.3425177335739136\n",
      "time elapsed: 3.6405s\n",
      "total val loss:0.3414875566959381\n",
      "----------This is the 73 epoch training---------\n",
      "total train loss:0.3414875566959381\n",
      "time elapsed: 3.6715s\n",
      "total val loss:0.3404599726200104\n",
      "----------This is the 74 epoch training---------\n",
      "total train loss:0.3404599726200104\n",
      "time elapsed: 3.7055s\n",
      "total val loss:0.33943408727645874\n",
      "----------This is the 75 epoch training---------\n",
      "total train loss:0.33943408727645874\n",
      "time elapsed: 3.7330s\n",
      "total val loss:0.33841076493263245\n",
      "----------This is the 76 epoch training---------\n",
      "total train loss:0.33841076493263245\n",
      "time elapsed: 3.7650s\n",
      "total val loss:0.3373894989490509\n",
      "----------This is the 77 epoch training---------\n",
      "total train loss:0.3373894989490509\n",
      "time elapsed: 3.7970s\n",
      "total val loss:0.3363707363605499\n",
      "----------This is the 78 epoch training---------\n",
      "total train loss:0.3363707363605499\n",
      "time elapsed: 3.8310s\n",
      "total val loss:0.33535391092300415\n",
      "----------This is the 79 epoch training---------\n",
      "total train loss:0.33535391092300415\n",
      "time elapsed: 3.8599s\n",
      "total val loss:0.33433961868286133\n",
      "----------This is the 80 epoch training---------\n",
      "total train loss:0.33433961868286133\n",
      "time elapsed: 3.8919s\n",
      "total val loss:0.3333275318145752\n",
      "----------This is the 81 epoch training---------\n",
      "total train loss:0.3333275318145752\n",
      "time elapsed: 3.9209s\n",
      "total val loss:0.3323177397251129\n",
      "----------This is the 82 epoch training---------\n",
      "total train loss:0.3323177397251129\n",
      "time elapsed: 3.9539s\n",
      "total val loss:0.33131012320518494\n",
      "----------This is the 83 epoch training---------\n",
      "total train loss:0.33131012320518494\n",
      "time elapsed: 3.9939s\n",
      "total val loss:0.33030447363853455\n",
      "----------This is the 84 epoch training---------\n",
      "total train loss:0.33030447363853455\n",
      "time elapsed: 4.0238s\n",
      "total val loss:0.329301118850708\n",
      "----------This is the 85 epoch training---------\n",
      "total train loss:0.329301118850708\n",
      "time elapsed: 4.0558s\n",
      "total val loss:0.3283001184463501\n",
      "----------This is the 86 epoch training---------\n",
      "total train loss:0.3283001184463501\n",
      "time elapsed: 4.0858s\n",
      "total val loss:0.32730111479759216\n",
      "----------This is the 87 epoch training---------\n",
      "total train loss:0.32730111479759216\n",
      "time elapsed: 4.1148s\n",
      "total val loss:0.32630446553230286\n",
      "----------This is the 88 epoch training---------\n",
      "total train loss:0.32630446553230286\n",
      "time elapsed: 4.1478s\n",
      "total val loss:0.3253098428249359\n",
      "----------This is the 89 epoch training---------\n",
      "total train loss:0.3253098428249359\n",
      "time elapsed: 4.1798s\n",
      "total val loss:0.32431772351264954\n",
      "----------This is the 90 epoch training---------\n",
      "total train loss:0.32431772351264954\n",
      "time elapsed: 4.2067s\n",
      "total val loss:0.3233274519443512\n",
      "----------This is the 91 epoch training---------\n",
      "total train loss:0.3233274519443512\n",
      "time elapsed: 4.2407s\n",
      "total val loss:0.32233983278274536\n",
      "----------This is the 92 epoch training---------\n",
      "total train loss:0.32233983278274536\n",
      "time elapsed: 4.2747s\n",
      "total val loss:0.3213540017604828\n",
      "----------This is the 93 epoch training---------\n",
      "total train loss:0.3213540017604828\n",
      "time elapsed: 4.2997s\n",
      "total val loss:0.32037055492401123\n",
      "----------This is the 94 epoch training---------\n",
      "total train loss:0.32037055492401123\n",
      "time elapsed: 4.3277s\n",
      "total val loss:0.31938910484313965\n",
      "----------This is the 95 epoch training---------\n",
      "total train loss:0.31938910484313965\n",
      "time elapsed: 4.3547s\n",
      "total val loss:0.31840986013412476\n",
      "----------This is the 96 epoch training---------\n",
      "total train loss:0.31840986013412476\n",
      "time elapsed: 4.3916s\n",
      "total val loss:0.31743288040161133\n",
      "----------This is the 97 epoch training---------\n",
      "total train loss:0.31743288040161133\n",
      "time elapsed: 4.4236s\n",
      "total val loss:0.3164580166339874\n",
      "----------This is the 98 epoch training---------\n",
      "total train loss:0.3164580166339874\n",
      "time elapsed: 4.4486s\n",
      "total val loss:0.3154854476451874\n",
      "----------This is the 99 epoch training---------\n",
      "total train loss:0.3154854476451874\n",
      "time elapsed: 4.4786s\n",
      "total val loss:0.3145146369934082\n",
      "----------This is the 100 epoch training---------\n",
      "total train loss:0.3145146369934082\n",
      "time elapsed: 4.5096s\n",
      "total val loss:0.31354638934135437\n",
      "----------This is the 101 epoch training---------\n",
      "total train loss:0.31354638934135437\n",
      "time elapsed: 4.5476s\n",
      "total val loss:0.3125799894332886\n",
      "----------This is the 102 epoch training---------\n",
      "total train loss:0.3125799894332886\n",
      "time elapsed: 4.5735s\n",
      "total val loss:0.311615914106369\n",
      "----------This is the 103 epoch training---------\n",
      "total train loss:0.311615914106369\n",
      "time elapsed: 4.6055s\n",
      "total val loss:0.3106538951396942\n",
      "----------This is the 104 epoch training---------\n",
      "total train loss:0.3106538951396942\n",
      "time elapsed: 4.6415s\n",
      "total val loss:0.3096940517425537\n",
      "----------This is the 105 epoch training---------\n",
      "total train loss:0.3096940517425537\n",
      "time elapsed: 4.6735s\n",
      "total val loss:0.3087354004383087\n",
      "----------This is the 106 epoch training---------\n",
      "total train loss:0.3087354004383087\n",
      "time elapsed: 4.7105s\n",
      "total val loss:0.30777376890182495\n",
      "----------This is the 107 epoch training---------\n",
      "total train loss:0.30777376890182495\n",
      "time elapsed: 4.7414s\n",
      "total val loss:0.3068138659000397\n",
      "----------This is the 108 epoch training---------\n",
      "total train loss:0.3068138659000397\n",
      "time elapsed: 4.7684s\n",
      "total val loss:0.305855393409729\n",
      "----------This is the 109 epoch training---------\n",
      "total train loss:0.305855393409729\n",
      "time elapsed: 4.7944s\n",
      "total val loss:0.3048989772796631\n",
      "----------This is the 110 epoch training---------\n",
      "total train loss:0.3048989772796631\n",
      "time elapsed: 4.8274s\n",
      "total val loss:0.30394411087036133\n",
      "----------This is the 111 epoch training---------\n",
      "total train loss:0.30394411087036133\n",
      "time elapsed: 4.8564s\n",
      "total val loss:0.3029909133911133\n",
      "----------This is the 112 epoch training---------\n",
      "total train loss:0.3029909133911133\n",
      "time elapsed: 4.8914s\n",
      "total val loss:0.30203986167907715\n",
      "----------This is the 113 epoch training---------\n",
      "total train loss:0.30203986167907715\n",
      "time elapsed: 4.9273s\n",
      "total val loss:0.3010902404785156\n",
      "----------This is the 114 epoch training---------\n",
      "total train loss:0.3010902404785156\n",
      "time elapsed: 4.9673s\n",
      "total val loss:0.30014315247535706\n",
      "----------This is the 115 epoch training---------\n",
      "total train loss:0.30014315247535706\n",
      "time elapsed: 4.9983s\n",
      "total val loss:0.2991977632045746\n",
      "----------This is the 116 epoch training---------\n",
      "total train loss:0.2991977632045746\n",
      "time elapsed: 5.0263s\n",
      "total val loss:0.2982542812824249\n",
      "----------This is the 117 epoch training---------\n",
      "total train loss:0.2982542812824249\n",
      "time elapsed: 5.0593s\n",
      "total val loss:0.29731249809265137\n",
      "----------This is the 118 epoch training---------\n",
      "total train loss:0.29731249809265137\n",
      "time elapsed: 5.0912s\n",
      "total val loss:0.2963731586933136\n",
      "----------This is the 119 epoch training---------\n",
      "total train loss:0.2963731586933136\n",
      "time elapsed: 5.1162s\n",
      "total val loss:0.29543548822402954\n",
      "----------This is the 120 epoch training---------\n",
      "total train loss:0.29543548822402954\n",
      "time elapsed: 5.1442s\n",
      "total val loss:0.29450005292892456\n",
      "----------This is the 121 epoch training---------\n",
      "total train loss:0.29450005292892456\n",
      "time elapsed: 5.1742s\n",
      "total val loss:0.2935667037963867\n",
      "----------This is the 122 epoch training---------\n",
      "total train loss:0.2935667037963867\n",
      "time elapsed: 5.2002s\n",
      "total val loss:0.29263535141944885\n",
      "----------This is the 123 epoch training---------\n",
      "total train loss:0.29263535141944885\n",
      "time elapsed: 5.2332s\n",
      "total val loss:0.2917061746120453\n",
      "----------This is the 124 epoch training---------\n",
      "total train loss:0.2917061746120453\n",
      "time elapsed: 5.2581s\n",
      "total val loss:0.29077887535095215\n",
      "----------This is the 125 epoch training---------\n",
      "total train loss:0.29077887535095215\n",
      "time elapsed: 5.2831s\n",
      "total val loss:0.2898538112640381\n",
      "----------This is the 126 epoch training---------\n",
      "total train loss:0.2898538112640381\n",
      "time elapsed: 5.3141s\n",
      "total val loss:0.28893062472343445\n",
      "----------This is the 127 epoch training---------\n",
      "total train loss:0.28893062472343445\n",
      "time elapsed: 5.3421s\n",
      "total val loss:0.28800973296165466\n",
      "----------This is the 128 epoch training---------\n",
      "total train loss:0.28800973296165466\n",
      "time elapsed: 5.3771s\n",
      "total val loss:0.28709062933921814\n",
      "----------This is the 129 epoch training---------\n",
      "total train loss:0.28709062933921814\n",
      "time elapsed: 5.4071s\n",
      "total val loss:0.2861725389957428\n",
      "----------This is the 130 epoch training---------\n",
      "total train loss:0.2861725389957428\n",
      "time elapsed: 5.4350s\n",
      "total val loss:0.28525593876838684\n",
      "----------This is the 131 epoch training---------\n",
      "total train loss:0.28525593876838684\n",
      "time elapsed: 5.4670s\n",
      "total val loss:0.28434136509895325\n",
      "----------This is the 132 epoch training---------\n",
      "total train loss:0.28434136509895325\n",
      "time elapsed: 5.5060s\n",
      "total val loss:0.2834283709526062\n",
      "----------This is the 133 epoch training---------\n",
      "total train loss:0.2834283709526062\n",
      "time elapsed: 5.5410s\n",
      "total val loss:0.28251737356185913\n",
      "----------This is the 134 epoch training---------\n",
      "total train loss:0.28251737356185913\n",
      "time elapsed: 5.5760s\n",
      "total val loss:0.2816084325313568\n",
      "----------This is the 135 epoch training---------\n",
      "total train loss:0.2816084325313568\n",
      "time elapsed: 5.6269s\n",
      "total val loss:0.2807011008262634\n",
      "----------This is the 136 epoch training---------\n",
      "total train loss:0.2807011008262634\n",
      "time elapsed: 5.6649s\n",
      "total val loss:0.27979615330696106\n",
      "----------This is the 137 epoch training---------\n",
      "total train loss:0.27979615330696106\n",
      "time elapsed: 5.7049s\n",
      "total val loss:0.2788929045200348\n",
      "----------This is the 138 epoch training---------\n",
      "total train loss:0.2788929045200348\n",
      "time elapsed: 5.7439s\n",
      "total val loss:0.27799180150032043\n",
      "----------This is the 139 epoch training---------\n",
      "total train loss:0.27799180150032043\n",
      "time elapsed: 5.7818s\n",
      "total val loss:0.27709290385246277\n",
      "----------This is the 140 epoch training---------\n",
      "total train loss:0.27709290385246277\n",
      "time elapsed: 5.8198s\n",
      "total val loss:0.2761957347393036\n",
      "----------This is the 141 epoch training---------\n",
      "total train loss:0.2761957347393036\n",
      "time elapsed: 5.8548s\n",
      "total val loss:0.2753007411956787\n",
      "----------This is the 142 epoch training---------\n",
      "total train loss:0.2753007411956787\n",
      "time elapsed: 5.8888s\n",
      "total val loss:0.2744106650352478\n",
      "----------This is the 143 epoch training---------\n",
      "total train loss:0.2744106650352478\n",
      "time elapsed: 5.9278s\n",
      "total val loss:0.2735254466533661\n",
      "----------This is the 144 epoch training---------\n",
      "total train loss:0.2735254466533661\n",
      "time elapsed: 5.9707s\n",
      "total val loss:0.27264222502708435\n",
      "----------This is the 145 epoch training---------\n",
      "total train loss:0.27264222502708435\n",
      "time elapsed: 6.0097s\n",
      "total val loss:0.271761417388916\n",
      "----------This is the 146 epoch training---------\n",
      "total train loss:0.271761417388916\n",
      "time elapsed: 6.0447s\n",
      "total val loss:0.27088284492492676\n",
      "----------This is the 147 epoch training---------\n",
      "total train loss:0.27088284492492676\n",
      "time elapsed: 6.0737s\n",
      "total val loss:0.27000629901885986\n",
      "----------This is the 148 epoch training---------\n",
      "total train loss:0.27000629901885986\n",
      "time elapsed: 6.1207s\n",
      "total val loss:0.2691318690776825\n",
      "----------This is the 149 epoch training---------\n",
      "total train loss:0.2691318690776825\n",
      "time elapsed: 6.1526s\n",
      "total val loss:0.26825985312461853\n",
      "----------This is the 150 epoch training---------\n",
      "total train loss:0.26825985312461853\n",
      "time elapsed: 6.1926s\n",
      "total val loss:0.26738986372947693\n",
      "----------This is the 151 epoch training---------\n",
      "total train loss:0.26738986372947693\n",
      "time elapsed: 6.2336s\n",
      "total val loss:0.26652196049690247\n",
      "----------This is the 152 epoch training---------\n",
      "total train loss:0.26652196049690247\n",
      "time elapsed: 6.2716s\n",
      "total val loss:0.2656620442867279\n",
      "----------This is the 153 epoch training---------\n",
      "total train loss:0.2656620442867279\n",
      "time elapsed: 6.3105s\n",
      "total val loss:0.2648041248321533\n",
      "----------This is the 154 epoch training---------\n",
      "total train loss:0.2648041248321533\n",
      "time elapsed: 6.3435s\n",
      "total val loss:0.26394855976104736\n",
      "----------This is the 155 epoch training---------\n",
      "total train loss:0.26394855976104736\n",
      "time elapsed: 6.4005s\n",
      "total val loss:0.26309525966644287\n",
      "----------This is the 156 epoch training---------\n",
      "total train loss:0.26309525966644287\n",
      "time elapsed: 6.4335s\n",
      "total val loss:0.26224395632743835\n",
      "----------This is the 157 epoch training---------\n",
      "total train loss:0.26224395632743835\n",
      "time elapsed: 6.4605s\n",
      "total val loss:0.26139509677886963\n",
      "----------This is the 158 epoch training---------\n",
      "total train loss:0.26139509677886963\n",
      "time elapsed: 6.4924s\n",
      "total val loss:0.2605482041835785\n",
      "----------This is the 159 epoch training---------\n",
      "total train loss:0.2605482041835785\n",
      "time elapsed: 6.5295s\n",
      "total val loss:0.2597036361694336\n",
      "----------This is the 160 epoch training---------\n",
      "total train loss:0.2597036361694336\n",
      "time elapsed: 6.5725s\n",
      "total val loss:0.25886109471321106\n",
      "----------This is the 161 epoch training---------\n",
      "total train loss:0.25886109471321106\n",
      "time elapsed: 6.6055s\n",
      "total val loss:0.25802066922187805\n",
      "----------This is the 162 epoch training---------\n",
      "total train loss:0.25802066922187805\n",
      "time elapsed: 6.6455s\n",
      "total val loss:0.25719916820526123\n",
      "----------This is the 163 epoch training---------\n",
      "total train loss:0.25719916820526123\n",
      "time elapsed: 6.6805s\n",
      "total val loss:0.2563776969909668\n",
      "----------This is the 164 epoch training---------\n",
      "total train loss:0.2563776969909668\n",
      "time elapsed: 6.7125s\n",
      "total val loss:0.25555598735809326\n",
      "----------This is the 165 epoch training---------\n",
      "total train loss:0.25555598735809326\n",
      "time elapsed: 6.7575s\n",
      "total val loss:0.25473448634147644\n",
      "----------This is the 166 epoch training---------\n",
      "total train loss:0.25473448634147644\n",
      "time elapsed: 6.7995s\n",
      "total val loss:0.2539134621620178\n",
      "----------This is the 167 epoch training---------\n",
      "total train loss:0.2539134621620178\n",
      "time elapsed: 6.8265s\n",
      "total val loss:0.25309309363365173\n",
      "----------This is the 168 epoch training---------\n",
      "total train loss:0.25309309363365173\n",
      "time elapsed: 6.8525s\n",
      "total val loss:0.2522735893726349\n",
      "----------This is the 169 epoch training---------\n",
      "total train loss:0.2522735893726349\n",
      "time elapsed: 6.8925s\n",
      "total val loss:0.25145503878593445\n",
      "----------This is the 170 epoch training---------\n",
      "total train loss:0.25145503878593445\n",
      "time elapsed: 6.9225s\n",
      "total val loss:0.25063765048980713\n",
      "----------This is the 171 epoch training---------\n",
      "total train loss:0.25063765048980713\n",
      "time elapsed: 6.9485s\n",
      "total val loss:0.2498215138912201\n",
      "----------This is the 172 epoch training---------\n",
      "total train loss:0.2498215138912201\n",
      "time elapsed: 6.9785s\n",
      "total val loss:0.249006986618042\n",
      "----------This is the 173 epoch training---------\n",
      "total train loss:0.249006986618042\n",
      "time elapsed: 7.0215s\n",
      "total val loss:0.24819374084472656\n",
      "----------This is the 174 epoch training---------\n",
      "total train loss:0.24819374084472656\n",
      "time elapsed: 7.0555s\n",
      "total val loss:0.2473859041929245\n",
      "----------This is the 175 epoch training---------\n",
      "total train loss:0.2473859041929245\n",
      "time elapsed: 7.0875s\n",
      "total val loss:0.2465871423482895\n",
      "----------This is the 176 epoch training---------\n",
      "total train loss:0.2465871423482895\n",
      "time elapsed: 7.1285s\n",
      "total val loss:0.24578893184661865\n",
      "----------This is the 177 epoch training---------\n",
      "total train loss:0.24578893184661865\n",
      "time elapsed: 7.1955s\n",
      "total val loss:0.24499104917049408\n",
      "----------This is the 178 epoch training---------\n",
      "total train loss:0.24499104917049408\n",
      "time elapsed: 7.2255s\n",
      "total val loss:0.2441941648721695\n",
      "----------This is the 179 epoch training---------\n",
      "total train loss:0.2441941648721695\n",
      "time elapsed: 7.2635s\n",
      "total val loss:0.2433982789516449\n",
      "----------This is the 180 epoch training---------\n",
      "total train loss:0.2433982789516449\n",
      "time elapsed: 7.3035s\n",
      "total val loss:0.24260343611240387\n",
      "----------This is the 181 epoch training---------\n",
      "total train loss:0.24260343611240387\n",
      "time elapsed: 7.3395s\n",
      "total val loss:0.24180977046489716\n",
      "----------This is the 182 epoch training---------\n",
      "total train loss:0.24180977046489716\n",
      "time elapsed: 7.3905s\n",
      "total val loss:0.2410176396369934\n",
      "----------This is the 183 epoch training---------\n",
      "total train loss:0.2410176396369934\n",
      "time elapsed: 7.4235s\n",
      "total val loss:0.2402275949716568\n",
      "----------This is the 184 epoch training---------\n",
      "total train loss:0.2402275949716568\n",
      "time elapsed: 7.4625s\n",
      "total val loss:0.23944658041000366\n",
      "----------This is the 185 epoch training---------\n",
      "total train loss:0.23944658041000366\n",
      "time elapsed: 7.5045s\n",
      "total val loss:0.23866675794124603\n",
      "----------This is the 186 epoch training---------\n",
      "total train loss:0.23866675794124603\n",
      "time elapsed: 7.5475s\n",
      "total val loss:0.23788729310035706\n",
      "----------This is the 187 epoch training---------\n",
      "total train loss:0.23788729310035706\n",
      "time elapsed: 7.5895s\n",
      "total val loss:0.2371084988117218\n",
      "----------This is the 188 epoch training---------\n",
      "total train loss:0.2371084988117218\n",
      "time elapsed: 7.6305s\n",
      "total val loss:0.2363307774066925\n",
      "----------This is the 189 epoch training---------\n",
      "total train loss:0.2363307774066925\n",
      "time elapsed: 7.6745s\n",
      "total val loss:0.2355542778968811\n",
      "----------This is the 190 epoch training---------\n",
      "total train loss:0.2355542778968811\n",
      "time elapsed: 7.7175s\n",
      "total val loss:0.23477882146835327\n",
      "----------This is the 191 epoch training---------\n",
      "total train loss:0.23477882146835327\n",
      "time elapsed: 7.7595s\n",
      "total val loss:0.23400606215000153\n",
      "----------This is the 192 epoch training---------\n",
      "total train loss:0.23400606215000153\n",
      "time elapsed: 7.8005s\n",
      "total val loss:0.2332388311624527\n",
      "----------This is the 193 epoch training---------\n",
      "total train loss:0.2332388311624527\n",
      "time elapsed: 7.8426s\n",
      "total val loss:0.23247259855270386\n",
      "----------This is the 194 epoch training---------\n",
      "total train loss:0.23247259855270386\n",
      "time elapsed: 7.8856s\n",
      "total val loss:0.2317073494195938\n",
      "----------This is the 195 epoch training---------\n",
      "total train loss:0.2317073494195938\n",
      "time elapsed: 7.9306s\n",
      "total val loss:0.2309432178735733\n",
      "----------This is the 196 epoch training---------\n",
      "total train loss:0.2309432178735733\n",
      "time elapsed: 7.9746s\n",
      "total val loss:0.23018020391464233\n",
      "----------This is the 197 epoch training---------\n",
      "total train loss:0.23018020391464233\n",
      "time elapsed: 8.0116s\n",
      "total val loss:0.22942081093788147\n",
      "----------This is the 198 epoch training---------\n",
      "total train loss:0.22942081093788147\n",
      "time elapsed: 8.0556s\n",
      "total val loss:0.22866417467594147\n",
      "----------This is the 199 epoch training---------\n",
      "total train loss:0.22866417467594147\n",
      "time elapsed: 8.0946s\n",
      "total val loss:0.22790823876857758\n",
      "----------This is the 200 epoch training---------\n",
      "total train loss:0.22790823876857758\n",
      "time elapsed: 8.1306s\n",
      "total val loss:0.22715352475643158\n",
      "----------This is the 201 epoch training---------\n",
      "total train loss:0.22715352475643158\n",
      "time elapsed: 8.1596s\n",
      "total val loss:0.2263997495174408\n",
      "----------This is the 202 epoch training---------\n",
      "total train loss:0.2263997495174408\n",
      "time elapsed: 8.1926s\n",
      "total val loss:0.22565141320228577\n",
      "----------This is the 203 epoch training---------\n",
      "total train loss:0.22565141320228577\n",
      "time elapsed: 8.2316s\n",
      "total val loss:0.2249041497707367\n",
      "----------This is the 204 epoch training---------\n",
      "total train loss:0.2249041497707367\n",
      "time elapsed: 8.2686s\n",
      "total val loss:0.2241578996181488\n",
      "----------This is the 205 epoch training---------\n",
      "total train loss:0.2241578996181488\n",
      "time elapsed: 8.3021s\n",
      "total val loss:0.22341232001781464\n",
      "----------This is the 206 epoch training---------\n",
      "total train loss:0.22341232001781464\n",
      "time elapsed: 8.3406s\n",
      "total val loss:0.2226676642894745\n",
      "----------This is the 207 epoch training---------\n",
      "total train loss:0.2226676642894745\n",
      "time elapsed: 8.3736s\n",
      "total val loss:0.22192591428756714\n",
      "----------This is the 208 epoch training---------\n",
      "total train loss:0.22192591428756714\n",
      "time elapsed: 8.4136s\n",
      "total val loss:0.2211877405643463\n",
      "----------This is the 209 epoch training---------\n",
      "total train loss:0.2211877405643463\n",
      "time elapsed: 8.4455s\n",
      "total val loss:0.2204502671957016\n",
      "----------This is the 210 epoch training---------\n",
      "total train loss:0.2204502671957016\n",
      "time elapsed: 8.4806s\n",
      "total val loss:0.21971353888511658\n",
      "----------This is the 211 epoch training---------\n",
      "total train loss:0.21971353888511658\n",
      "time elapsed: 8.5206s\n",
      "total val loss:0.21897903084754944\n",
      "----------This is the 212 epoch training---------\n",
      "total train loss:0.21897903084754944\n",
      "time elapsed: 8.5605s\n",
      "total val loss:0.21824763715267181\n",
      "----------This is the 213 epoch training---------\n",
      "total train loss:0.21824763715267181\n",
      "time elapsed: 8.5885s\n",
      "total val loss:0.2175169438123703\n",
      "----------This is the 214 epoch training---------\n",
      "total train loss:0.2175169438123703\n",
      "time elapsed: 8.6215s\n",
      "total val loss:0.21678826212882996\n",
      "----------This is the 215 epoch training---------\n",
      "total train loss:0.21678826212882996\n",
      "time elapsed: 8.6565s\n",
      "total val loss:0.21606165170669556\n",
      "----------This is the 216 epoch training---------\n",
      "total train loss:0.21606165170669556\n",
      "time elapsed: 8.6955s\n",
      "total val loss:0.2153359204530716\n",
      "----------This is the 217 epoch training---------\n",
      "total train loss:0.2153359204530716\n",
      "time elapsed: 8.7345s\n",
      "total val loss:0.21461404860019684\n",
      "----------This is the 218 epoch training---------\n",
      "total train loss:0.21461404860019684\n",
      "time elapsed: 8.7726s\n",
      "total val loss:0.21389293670654297\n",
      "----------This is the 219 epoch training---------\n",
      "total train loss:0.21389293670654297\n",
      "time elapsed: 8.8095s\n",
      "total val loss:0.21317274868488312\n",
      "----------This is the 220 epoch training---------\n",
      "total train loss:0.21317274868488312\n",
      "time elapsed: 8.8435s\n",
      "total val loss:0.21245373785495758\n",
      "----------This is the 221 epoch training---------\n",
      "total train loss:0.21245373785495758\n",
      "time elapsed: 8.8845s\n",
      "total val loss:0.21173788607120514\n",
      "----------This is the 222 epoch training---------\n",
      "total train loss:0.21173788607120514\n",
      "time elapsed: 8.9175s\n",
      "total val loss:0.21102339029312134\n",
      "----------This is the 223 epoch training---------\n",
      "total train loss:0.21102339029312134\n",
      "time elapsed: 8.9575s\n",
      "total val loss:0.21031077206134796\n",
      "----------This is the 224 epoch training---------\n",
      "total train loss:0.21031077206134796\n",
      "time elapsed: 8.9975s\n",
      "total val loss:0.20959961414337158\n",
      "----------This is the 225 epoch training---------\n",
      "total train loss:0.20959961414337158\n",
      "time elapsed: 9.0415s\n",
      "total val loss:0.2088906615972519\n",
      "----------This is the 226 epoch training---------\n",
      "total train loss:0.2088906615972519\n",
      "time elapsed: 9.0835s\n",
      "total val loss:0.20818275213241577\n",
      "----------This is the 227 epoch training---------\n",
      "total train loss:0.20818275213241577\n",
      "time elapsed: 9.1285s\n",
      "total val loss:0.20747792720794678\n",
      "----------This is the 228 epoch training---------\n",
      "total train loss:0.20747792720794678\n",
      "time elapsed: 9.1715s\n",
      "total val loss:0.206774041056633\n",
      "----------This is the 229 epoch training---------\n",
      "total train loss:0.206774041056633\n",
      "time elapsed: 9.2164s\n",
      "total val loss:0.20607075095176697\n",
      "----------This is the 230 epoch training---------\n",
      "total train loss:0.20607075095176697\n",
      "time elapsed: 9.2594s\n",
      "total val loss:0.20537088811397552\n",
      "----------This is the 231 epoch training---------\n",
      "total train loss:0.20537088811397552\n",
      "time elapsed: 9.3015s\n",
      "total val loss:0.20467273890972137\n",
      "----------This is the 232 epoch training---------\n",
      "total train loss:0.20467273890972137\n",
      "time elapsed: 9.3445s\n",
      "total val loss:0.20397523045539856\n",
      "----------This is the 233 epoch training---------\n",
      "total train loss:0.20397523045539856\n",
      "time elapsed: 9.3865s\n",
      "total val loss:0.2032787948846817\n",
      "----------This is the 234 epoch training---------\n",
      "total train loss:0.2032787948846817\n",
      "time elapsed: 9.4304s\n",
      "total val loss:0.20258566737174988\n",
      "----------This is the 235 epoch training---------\n",
      "total train loss:0.20258566737174988\n",
      "time elapsed: 9.4724s\n",
      "total val loss:0.20189400017261505\n",
      "----------This is the 236 epoch training---------\n",
      "total train loss:0.20189400017261505\n",
      "time elapsed: 9.5214s\n",
      "total val loss:0.2012031376361847\n",
      "----------This is the 237 epoch training---------\n",
      "total train loss:0.2012031376361847\n",
      "time elapsed: 9.5615s\n",
      "total val loss:0.20051328837871552\n",
      "----------This is the 238 epoch training---------\n",
      "total train loss:0.20051328837871552\n",
      "time elapsed: 9.5995s\n",
      "total val loss:0.1998278945684433\n",
      "----------This is the 239 epoch training---------\n",
      "total train loss:0.1998278945684433\n",
      "time elapsed: 9.6395s\n",
      "total val loss:0.1991434395313263\n",
      "----------This is the 240 epoch training---------\n",
      "total train loss:0.1991434395313263\n",
      "time elapsed: 9.6804s\n",
      "total val loss:0.19845980405807495\n",
      "----------This is the 241 epoch training---------\n",
      "total train loss:0.19845980405807495\n",
      "time elapsed: 9.7194s\n",
      "total val loss:0.19777743518352509\n",
      "----------This is the 242 epoch training---------\n",
      "total train loss:0.19777743518352509\n",
      "time elapsed: 9.7574s\n",
      "total val loss:0.1970958411693573\n",
      "----------This is the 243 epoch training---------\n",
      "total train loss:0.1970958411693573\n",
      "time elapsed: 9.7964s\n",
      "total val loss:0.19641798734664917\n",
      "----------This is the 244 epoch training---------\n",
      "total train loss:0.19641798734664917\n",
      "time elapsed: 9.8785s\n",
      "total val loss:0.19574226438999176\n",
      "----------This is the 245 epoch training---------\n",
      "total train loss:0.19574226438999176\n",
      "time elapsed: 9.9125s\n",
      "total val loss:0.19506707787513733\n",
      "----------This is the 246 epoch training---------\n",
      "total train loss:0.19506707787513733\n",
      "time elapsed: 9.9584s\n",
      "total val loss:0.19439278542995453\n",
      "----------This is the 247 epoch training---------\n",
      "total train loss:0.19439278542995453\n",
      "time elapsed: 9.9935s\n",
      "total val loss:0.19372034072875977\n",
      "----------This is the 248 epoch training---------\n",
      "total train loss:0.19372034072875977\n",
      "time elapsed: 10.0335s\n",
      "total val loss:0.19305123388767242\n",
      "----------This is the 249 epoch training---------\n",
      "total train loss:0.19305123388767242\n",
      "time elapsed: 10.0885s\n",
      "total val loss:0.1923869252204895\n",
      "----------This is the 250 epoch training---------\n",
      "total train loss:0.1923869252204895\n",
      "time elapsed: 10.1294s\n",
      "total val loss:0.19172365963459015\n",
      "----------This is the 251 epoch training---------\n",
      "total train loss:0.19172365963459015\n",
      "time elapsed: 10.1704s\n",
      "total val loss:0.19106435775756836\n",
      "----------This is the 252 epoch training---------\n",
      "total train loss:0.19106435775756836\n",
      "time elapsed: 10.2064s\n",
      "total val loss:0.1904064416885376\n",
      "----------This is the 253 epoch training---------\n",
      "total train loss:0.1904064416885376\n",
      "time elapsed: 10.2424s\n",
      "total val loss:0.18974921107292175\n",
      "----------This is the 254 epoch training---------\n",
      "total train loss:0.18974921107292175\n",
      "time elapsed: 10.2724s\n",
      "total val loss:0.1890926957130432\n",
      "----------This is the 255 epoch training---------\n",
      "total train loss:0.1890926957130432\n",
      "time elapsed: 10.3215s\n",
      "total val loss:0.1884409785270691\n",
      "----------This is the 256 epoch training---------\n",
      "total train loss:0.1884409785270691\n",
      "time elapsed: 10.3484s\n",
      "total val loss:0.18779036402702332\n",
      "----------This is the 257 epoch training---------\n",
      "total train loss:0.18779036402702332\n",
      "time elapsed: 10.3754s\n",
      "total val loss:0.18714110553264618\n",
      "----------This is the 258 epoch training---------\n",
      "total train loss:0.18714110553264618\n",
      "time elapsed: 10.4014s\n",
      "total val loss:0.18649274110794067\n",
      "----------This is the 259 epoch training---------\n",
      "total train loss:0.18649274110794067\n",
      "time elapsed: 10.4285s\n",
      "total val loss:0.185845285654068\n",
      "----------This is the 260 epoch training---------\n",
      "total train loss:0.185845285654068\n",
      "time elapsed: 10.4815s\n",
      "total val loss:0.18519940972328186\n",
      "----------This is the 261 epoch training---------\n",
      "total train loss:0.18519940972328186\n",
      "time elapsed: 10.5115s\n",
      "total val loss:0.1845581829547882\n",
      "----------This is the 262 epoch training---------\n",
      "total train loss:0.1845581829547882\n",
      "time elapsed: 10.5475s\n",
      "total val loss:0.1839180439710617\n",
      "----------This is the 263 epoch training---------\n",
      "total train loss:0.1839180439710617\n",
      "time elapsed: 10.5785s\n",
      "total val loss:0.18327832221984863\n",
      "----------This is the 264 epoch training---------\n",
      "total train loss:0.18327832221984863\n",
      "time elapsed: 10.6416s\n",
      "total val loss:0.18263980746269226\n",
      "----------This is the 265 epoch training---------\n",
      "total train loss:0.18263980746269226\n",
      "time elapsed: 10.6686s\n",
      "total val loss:0.18200181424617767\n",
      "----------This is the 266 epoch training---------\n",
      "total train loss:0.18200181424617767\n",
      "time elapsed: 10.6975s\n",
      "total val loss:0.1813676506280899\n",
      "----------This is the 267 epoch training---------\n",
      "total train loss:0.1813676506280899\n",
      "time elapsed: 10.7285s\n",
      "total val loss:0.18073582649230957\n",
      "----------This is the 268 epoch training---------\n",
      "total train loss:0.18073582649230957\n",
      "time elapsed: 10.7755s\n",
      "total val loss:0.18010491132736206\n",
      "----------This is the 269 epoch training---------\n",
      "total train loss:0.18010491132736206\n",
      "time elapsed: 10.8085s\n",
      "total val loss:0.17947505414485931\n",
      "----------This is the 270 epoch training---------\n",
      "total train loss:0.17947505414485931\n",
      "time elapsed: 10.8465s\n",
      "total val loss:0.17884621024131775\n",
      "----------This is the 271 epoch training---------\n",
      "total train loss:0.17884621024131775\n",
      "time elapsed: 10.8825s\n",
      "total val loss:0.17821858823299408\n",
      "----------This is the 272 epoch training---------\n",
      "total train loss:0.17821858823299408\n",
      "time elapsed: 10.9135s\n",
      "total val loss:0.17759329080581665\n",
      "----------This is the 273 epoch training---------\n",
      "total train loss:0.17759329080581665\n",
      "time elapsed: 10.9535s\n",
      "total val loss:0.1769714057445526\n",
      "----------This is the 274 epoch training---------\n",
      "total train loss:0.1769714057445526\n",
      "time elapsed: 10.9905s\n",
      "total val loss:0.17635075747966766\n",
      "----------This is the 275 epoch training---------\n",
      "total train loss:0.17635075747966766\n",
      "time elapsed: 11.0235s\n",
      "total val loss:0.17573034763336182\n",
      "----------This is the 276 epoch training---------\n",
      "total train loss:0.17573034763336182\n",
      "time elapsed: 11.0605s\n",
      "total val loss:0.17511282861232758\n",
      "----------This is the 277 epoch training---------\n",
      "total train loss:0.17511282861232758\n",
      "time elapsed: 11.0935s\n",
      "total val loss:0.1744968146085739\n",
      "----------This is the 278 epoch training---------\n",
      "total train loss:0.1744968146085739\n",
      "time elapsed: 11.1325s\n",
      "total val loss:0.1738816350698471\n",
      "----------This is the 279 epoch training---------\n",
      "total train loss:0.1738816350698471\n",
      "time elapsed: 11.1635s\n",
      "total val loss:0.1732674539089203\n",
      "----------This is the 280 epoch training---------\n",
      "total train loss:0.1732674539089203\n",
      "time elapsed: 11.1955s\n",
      "total val loss:0.17265494167804718\n",
      "----------This is the 281 epoch training---------\n",
      "total train loss:0.17265494167804718\n",
      "time elapsed: 11.2395s\n",
      "total val loss:0.1720454841852188\n",
      "----------This is the 282 epoch training---------\n",
      "total train loss:0.1720454841852188\n",
      "time elapsed: 11.2805s\n",
      "total val loss:0.17143680155277252\n",
      "----------This is the 283 epoch training---------\n",
      "total train loss:0.17143680155277252\n",
      "time elapsed: 11.3215s\n",
      "total val loss:0.17082983255386353\n",
      "----------This is the 284 epoch training---------\n",
      "total train loss:0.17082983255386353\n",
      "time elapsed: 11.3605s\n",
      "total val loss:0.17022529244422913\n",
      "----------This is the 285 epoch training---------\n",
      "total train loss:0.17022529244422913\n",
      "time elapsed: 11.4075s\n",
      "total val loss:0.16962049901485443\n",
      "----------This is the 286 epoch training---------\n",
      "total train loss:0.16962049901485443\n",
      "time elapsed: 11.4415s\n",
      "total val loss:0.1690165102481842\n",
      "----------This is the 287 epoch training---------\n",
      "total train loss:0.1690165102481842\n",
      "time elapsed: 11.4754s\n",
      "total val loss:0.1684141904115677\n",
      "----------This is the 288 epoch training---------\n",
      "total train loss:0.1684141904115677\n",
      "time elapsed: 11.5035s\n",
      "total val loss:0.16781289875507355\n",
      "----------This is the 289 epoch training---------\n",
      "total train loss:0.16781289875507355\n",
      "time elapsed: 11.5355s\n",
      "total val loss:0.16721317172050476\n",
      "----------This is the 290 epoch training---------\n",
      "total train loss:0.16721317172050476\n",
      "time elapsed: 11.5615s\n",
      "total val loss:0.1666146069765091\n",
      "----------This is the 291 epoch training---------\n",
      "total train loss:0.1666146069765091\n",
      "time elapsed: 11.6175s\n",
      "total val loss:0.16601653397083282\n",
      "----------This is the 292 epoch training---------\n",
      "total train loss:0.16601653397083282\n",
      "time elapsed: 11.6755s\n",
      "total val loss:0.1654217690229416\n",
      "----------This is the 293 epoch training---------\n",
      "total train loss:0.1654217690229416\n",
      "time elapsed: 11.7065s\n",
      "total val loss:0.16482822597026825\n",
      "----------This is the 294 epoch training---------\n",
      "total train loss:0.16482822597026825\n",
      "time elapsed: 11.7405s\n",
      "total val loss:0.16423554718494415\n",
      "----------This is the 295 epoch training---------\n",
      "total train loss:0.16423554718494415\n",
      "time elapsed: 11.7745s\n",
      "total val loss:0.16364386677742004\n",
      "----------This is the 296 epoch training---------\n",
      "total train loss:0.16364386677742004\n",
      "time elapsed: 11.8045s\n",
      "total val loss:0.16305290162563324\n",
      "----------This is the 297 epoch training---------\n",
      "total train loss:0.16305290162563324\n",
      "time elapsed: 11.8395s\n",
      "total val loss:0.16246525943279266\n",
      "----------This is the 298 epoch training---------\n",
      "total train loss:0.16246525943279266\n",
      "time elapsed: 11.8744s\n",
      "total val loss:0.16187867522239685\n",
      "----------This is the 299 epoch training---------\n",
      "total train loss:0.16187867522239685\n",
      "time elapsed: 11.9124s\n",
      "total val loss:0.16129323840141296\n",
      "----------This is the 300 epoch training---------\n",
      "total train loss:0.16129323840141296\n",
      "time elapsed: 11.9484s\n",
      "total val loss:0.1607082188129425\n"
     ]
    }
   ],
   "source": [
    "#define loss\n",
    "loss_fn = nn.MSELoss()\n",
    "loss_fn = loss_fn.cuda()\n",
    "\n",
    "#define optimizer\n",
    "lr = 1e-6\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr, eps=1e-8, weight_decay=1e-4)\n",
    "\n",
    "#traiing parameters\n",
    "total_train_step = 0\n",
    "total_val_step = 0\n",
    "epoch = 300\n",
    "\n",
    "#train\n",
    "start_time = time.time()\n",
    "for i in range(epoch):\n",
    "    print('----------This is the {} epoch training---------'.format(i+1))\n",
    "    train_loss = 0\n",
    "    \n",
    "    #below is for training\n",
    "    model.train()\n",
    "    for imgs, labels in train_dataloader: #for 1 epoch, 66 steps are needed (train data/ batch size)    \n",
    "        imgs, labels = imgs.to(device), labels.to(device)            \n",
    "        outputs = model(imgs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        #optimizer\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() #update weights        \n",
    "        total_train_step = total_train_step + 1 #step means iteration, 1 iter means train the number of (batch size) samples        \n",
    "        train_loss += loss.item()\n",
    "    print('total train loss:{}'.format(train_loss))\n",
    "    print('time elapsed: {:.4f}s'.format(time.time()-start_time))\n",
    "    \n",
    "    #below is for evaluation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_dataloader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_val_loss = total_val_loss + loss.item()             \n",
    "            total_val_step += 1 \n",
    "      \n",
    "    print('total val loss:{}'.format(total_val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> step3: predict the testing data after the model is fully trained (play with different parameter settings and fine tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2341]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use model.predict() to predict properties of interest of testing data\n",
    "example_test = np.random.rand(1,1,224,224)\n",
    "model(torch.from_numpy(example_test).float().to(device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
